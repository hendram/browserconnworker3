# Puppeteer Worker 3

This project is the **second worker (of three)** that scrapes URLs fed from a backend service (`chunkgeneratorforaimodel`) based on searched keywords.  
The workers run in parallel to share the scraping load, increasing the speed and efficiency of the process.

---

## ðŸ“¦ About This Package

- Functions as **3/3 of the scraper workload**.  
- Consumes jobs (**URLs + keywords**) from **Kafka**.  
- Uses **Puppeteer** to scrape the content of web pages.  
- Filters and extracts text that contains the searched keywords.  
- Sends results back to Kafka for further processing by the backend (`chunkgeneratorforaimodel`).  
- Designed to run together with two other workers for parallel scraping.  

---

## âš™ï¸ How It Works

### ðŸ”¹ Kafka Consumer
- Listens for messages on the **`topuppeteerworker`** topic.  
- Each message contains ~1/3 of the total URLs to be scraped.  

### ðŸ”¹ Scraping Process (`runScraper` in `puppeteerWorker.js`)
- ðŸ–¥ï¸ Launches Puppeteer in headless mode  
- ðŸŒ Visits each URL  
- ðŸ” Extracts page text  
- âœ… Checks for presence of keywords from `topicsArray`  
- ðŸ“¦ If found, pushes a result in the format:  

{
  "text": "page content",
  "metadata": {
    "url": "https://example.com",
    "date": "2025-09-11T10:00:00.000Z",
    "sourcekb": "external",
    "searched": "your_query"
  }
}

---

ðŸ’» Platform Requirements

At least 12 GB RAM

At least 4 CPU cores (Intel i5 or higher recommended)

Docker installed

---

ðŸš€ How to Run

Download the Docker image

**docker pull ghcr.io/hendram/puppeteerworker3**

Start the container

**docker run -it -d --network=host ghcr.io/hendram/puppeteerworker3 bash**

Find your container name

**docker ps**

Example: elastic_cori (your name will differ).

Enter the container

**docker exec -it pedantic_payne /bin/bash**

Run the service

**cd /home/browserconnworker3**
**node index.js**

---

ðŸ”§ Code Overview

index.js
ðŸš€ Starts the worker, consumes jobs from Kafka, runs the scraper, and sends results back.

kafka.js
ðŸ”Œ Handles Kafka connection, producer, and consumer setup.

puppeteerWorker.js
ðŸ•·ï¸ Contains the runScraper function, which:

ðŸ–¥ï¸ Launches Puppeteer (headless browser)

ðŸŒ Visits URLs

ðŸ” Extracts and filters page content

ðŸ“¦ Returns results with metadata

---

âœ¨ Features & Functionality

âœ”ï¸ Consumes jobs (URLs + keywords) from Kafka

âœ”ï¸ Uses Puppeteer to scrape web pages in headless mode

âœ”ï¸ Extracts page text and checks if it contains keywords

âœ”ï¸ Produces structured results with metadata (URL, date, source, searched keyword)

âœ”ï¸ Sends results back to Kafka for merging with outputs from other workers

âœ”ï¸ Designed for distributed, scalable, and fast scraping

---

ðŸ“¡ Data Flow

    A[chunkgeneratorforaimodel] -->|Jobs| B[Kafka: "topuppeteerworker"]

    B --> C[Puppeteer Worker 3 ðŸ•·ï¸]

    C -->|Results| D[Kafka: "frompuppeteerworker"]

    D --> E[Backend merges with Worker 1 & 2]
